{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Spam Classification\n",
    "This notebook illustrates classification of SMS as SPAM or NOT SPAM using Recurrent Neural Network and LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamonangan\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Hamonangan\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Hamonangan\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Hamonangan\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Hamonangan\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Hamonangan\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Hamonangan\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Hamonangan\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Hamonangan\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Hamonangan\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Hamonangan\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Hamonangan\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas\n",
    "import sklearn\n",
    "import pickle\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import learning_curve    \n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Subject: great nnews  hello , welcome to medzo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Subject: here ' s a hot play in motion  homela...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Subject: save your money buy getting this thin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Subject: undeliverable : home based business f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Subject: save your money buy getting this thin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Subject: naturally irresistible your corporate...      1\n",
       "1  Subject: the stock trading gunslinger  fanny i...      1\n",
       "2  Subject: unbelievable new homes made easy  im ...      1\n",
       "3  Subject: 4 color printing special  request add...      1\n",
       "4  Subject: do not have money , get software cds ...      1\n",
       "5  Subject: great nnews  hello , welcome to medzo...      1\n",
       "6  Subject: here ' s a hot play in motion  homela...      1\n",
       "7  Subject: save your money buy getting this thin...      1\n",
       "8  Subject: undeliverable : home based business f...      1\n",
       "9  Subject: save your money buy getting this thin...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/emails.csv',encoding='latin-1')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Lets save our labels and messages to text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(r'data\\messages.txt', data['text'].values, fmt='%s')\n",
    "np.savetxt(r'data\\labels.txt', data['label'].values, fmt='%s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/messages.txt', encoding=\"ISO-8859-1\") as f:\n",
    "    messages = f.read()\n",
    "with open('data/labels.txt',encoding=\"ISO-8859-1\") as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subje'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4358\n",
       "1    1368\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuations such a (. , !) etc and seperate using delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "all_text = ''.join([c for c in messages if c not in punctuation])\n",
    "messages = all_text.split('\\n')\n",
    "\n",
    "all_text = ' '.join(messages)\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject naturally irresistible your corporate identity  lt is really hard to recollect a company  the  market is full of suqgestions and the information isoverwhelminq  but a good  catchy logo  stylish statlonery and outstanding website  will make the task much easier   we do not promise that havinq ordered a iogo your  company will automaticaily become a world ieader  it isguite ciear that  without good products  effective business organization and practicable aim it  will be hotat nowadays mar\n",
      "\n",
      "\n",
      "['Subject', 'naturally', 'irresistible', 'your', 'corporate', 'identity', 'lt', 'is', 'really', 'hard', 'to', 'recollect', 'a', 'company', 'the', 'market', 'is', 'full', 'of', 'suqgestions']\n"
     ]
    }
   ],
   "source": [
    "print (all_text[:500])\n",
    "print (\"\\n\")\n",
    "print (words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our vocabulary and converting messages to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_words = Counter(words)\n",
    "sorted_split_words = sorted(split_words, key=split_words.get, reverse=True)\n",
    "vocab_to_int = {c : i for i, c in enumerate(sorted_split_words,1)}\n",
    "\n",
    "# Convert the reviews to integers, same shape as reviews list, but with integers\n",
    "messages_ints = []\n",
    "for message in messages:\n",
    "    messages_ints.append([vocab_to_int[i] for i in message.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'to', 'and', 'of', 'a', 'you', 'in', 'i', 'for', 'enron', 'on', 'is', 'ect', 'this', 'your', 'be', 'that', 'with', 'we', 'vince', 'will', 'have', 'at', 'from', 'it', 'are', 's', 'as', 'Subject', 'hou', 'com', 'by', 'or', 'if', 'am', 'please', '2000', 'kaminski', 'not', 'subject', 'me', 'would', 'our', 'can', 're', 'cc', 'j', 'my', 'an', '1']\n",
      "\n",
      "\n",
      "Subject naturally irresistible your corporate identity  lt is really hard to recollect a company  the  market is full of suqgestions and the information isoverwhelminq  but a good  catchy logo  stylish statlonery and outstanding website  will make the task much easier   we do not promise that havinq ordered a iogo your  company will automaticaily become a world ieader  it isguite ciear that  without good products  effective business organization and practicable aim it  will be hotat nowadays market  but we do promise that your marketing efforts  will become much more effective  here is the list of clear  benefits  creativeness  hand  made  original logos  specially done  to reflect your distinctive company image  convenience  logo and stationery  are provided in all formats  easy  to  use content management system letsyou  change your website content and even its structure  promptness  you  will see logo drafts within three business days  affordability  your  marketing break  through shouldn  t make gaps in your budget  100  satisfaction  guaranteed  we provide unlimited amount of changes with no extra fees for you to  be surethat you will love the result of this collaboration  have a look at our  portfolio                                                     not interested                                                       \n",
      "[29, 5795, 12225, 15, 516, 1775, 3893, 12, 457, 902, 2, 4386, 5, 111, 1, 117, 12, 399, 4, 6272, 3, 1, 67, 9136, 76, 5, 169, 4492, 829, 6035, 6036, 3, 1701, 289, 21, 125, 1, 2147, 225, 1847, 19, 60, 39, 1980, 17, 7965, 1702, 5, 4280, 15, 111, 21, 10940, 681, 5, 406, 5796, 25, 6532, 7966, 17, 488, 169, 349, 871, 77, 850, 3, 4493, 3034, 25, 21, 16, 4615, 4387, 117, 76, 19, 60, 1980, 17, 15, 454, 782, 21, 681, 225, 74, 871, 94, 12, 1, 172, 4, 1069, 1274, 4494, 1098, 313, 275, 2489, 3527, 553, 2, 2016, 15, 4281, 111, 1249, 1128, 829, 3, 1425, 26, 698, 7, 54, 2549, 834, 2, 166, 684, 118, 248, 4616, 446, 15, 289, 684, 3, 445, 182, 971, 4495, 6, 21, 127, 829, 4175, 265, 513, 77, 287, 4496, 15, 454, 1388, 215, 3528, 101, 125, 3734, 7, 15, 1341, 484, 2103, 1275, 19, 249, 1961, 767, 4, 429, 18, 114, 1475, 2037, 9, 6, 2, 16, 4617, 6, 21, 1290, 1, 924, 4, 14, 2384, 22, 5, 154, 23, 43, 688, 39, 217]\n",
      "\n",
      "\n",
      "1348\n",
      "189\n"
     ]
    }
   ],
   "source": [
    "print (sorted_split_words[:50])\n",
    "print (\"\\n\")\n",
    "print (messages[0])\n",
    "print (messages_ints[0])\n",
    "print (\"\\n\")\n",
    "print (len(messages[0]))\n",
    "print (len(messages_ints[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting labels to 0 and 1 - SPAM:1 and NOT SPAM:0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.split(\"\\n\")\n",
    "labels = np.array([0 if label == \"0\" else 1 for label in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length messages: 1\n",
      "Maximum message length: 5003\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "message_lens = Counter([len(x) for x in messages_ints])\n",
    "print(\"Zero-length messages: {}\".format(message_lens[0]))\n",
    "print(\"Maximum message length: {}\".format(max(message_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_ints = [message for message in messages_ints if (len(message)>0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding vectors with zeros so that all inputs are of same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 200\n",
    "num_messages = len(messages)\n",
    "features = np.zeros([num_messages, seq_len], dtype=int)\n",
    "for i, row in enumerate(messages_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,    29,     1,   503,   194, 22179, 22180,\n",
       "          12,  4890,    76, 22181,    39, 16808, 22182,     3, 22183,\n",
       "          73, 22184, 22185, 22186,    12, 22187,    39,    65,   517,\n",
       "       22188,  7495, 22189,   741, 22190,    33, 22191, 22192,   114,\n",
       "       22193,    12, 16809,    76,  5206,    39, 14158, 22194,     3,\n",
       "       12226,    73, 22195, 22196,  3357,    12, 22197,    39, 22198,\n",
       "         517,  1442,  5207, 22199,   741, 22200,    33, 22201, 22202,\n",
       "         114, 22203,    12, 22204,    76,  2834,    39,  4176, 22205,\n",
       "           3, 14159,    73, 22206, 22207, 22208,    12, 22209,    39,\n",
       "        7967,   517,  4751, 22210, 22211,   741, 22212,    33,  3981,\n",
       "        7968,   114])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into training, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(4581, 200) \n",
      "Validation set: \t(573, 200) \n",
      "Test set: \t\t(573, 200)\n",
      "\t\t\\Label Shapes:\n",
      "Train set: \t\t(4581,) \n",
      "Validation set: \t(573,) \n",
      "Test set: \t\t(573,)\n"
     ]
    }
   ],
   "source": [
    "split_frac1 = 0.8\n",
    "\n",
    "idx1 = int(len(features) * split_frac1)\n",
    "train_x, val_x = features[:idx1], features[idx1:]\n",
    "train_y, val_y = labels[:idx1], labels[idx1:]\n",
    "\n",
    "split_frac2 = 0.5\n",
    "idx2 = int(len(val_x) * split_frac2)\n",
    "val_x, test_x = val_x[:idx2], val_x[idx2:]\n",
    "val_y, test_y = val_y[:idx2], val_y[idx2:]\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n",
    "\n",
    "print(\"\\t\\t\\Label Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_y.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_y.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial prediction using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamonangan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93       573\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.87       573\n",
      "   macro avg       0.50      0.43      0.46       573\n",
      "weighted avg       1.00      0.87      0.93       573\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamonangan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Hamonangan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_x,train_y)\n",
    "p = clf.predict(val_x)\n",
    "\n",
    "print('Classification Report:\\n')\n",
    "print (classification_report(val_y,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lstm_size = 200\n",
    "lstm_layers = 1\n",
    "batch_size = 200 #Set batchSize ke 200\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating placeholder for inputs, labels and dropout rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(sorted_split_words)\n",
    "\n",
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "# Add nodes to the graph\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32, [None,None], name = \"inputs\")\n",
    "    labels_ = tf.placeholder(tf.int32, [None,None], name = \"labels\")\n",
    "    keep_prob = tf.placeholder(tf.float32, name = \"keep_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the embedding vectors (number of units in the embedding layer)\n",
    "embed_size = 300 \n",
    "\n",
    "with graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membuat cell LSTM untuk digunakan didalam RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-22-f9f1fc49d7c3>:3: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-22-f9f1fc49d7c3>:9: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    # Your basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to actually run the data through the RNN nodes. You can use tf.nn.dynamic_rnn to do this. You'd pass in the RNN cell you created (our multiple layered LSTM cell for instance), and the inputs to the network.\n",
    "\n",
    "Initial_state is the cell state that is passed between the hidden layers in successive time steps. We pass in our cell and the input to the cell, then it does the unrolling and everything else for us. It returns outputs for each time step and the final_state of the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-23-849734d2bee4>:2: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x000001A0EEED1F98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x000001A0EEED1F98>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x000001A0EEED1F98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x000001A0EEED1F98>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From C:\\Users\\Hamonangan\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Hamonangan\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001A0BBF27240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001A0BBF27240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001A0BBF27240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001A0BBF27240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only care about the final output, we'll be using that as our sentiment prediction. So we need to grab the last output with outputs[:, -1], the calculate the cost from that and labels_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A0F1002DA0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A0F1002DA0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A0F1002DA0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001A0F1002DA0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From C:\\Users\\Hamonangan\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with graph.as_default():\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menghitung tingkat akurasi dan prediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/16 Iteration: 5 Train loss: 0.079\n",
      "Epoch: 0/16 Iteration: 10 Train loss: 0.758\n",
      "Epoch: 0/16 Iteration: 15 Train loss: 0.364\n",
      "Epoch: 0/16 Iteration: 20 Train loss: 0.207\n",
      "Epoch: 1/16 Iteration: 25 Train loss: 0.233\n",
      "Val acc: 0.890\n",
      "Epoch: 1/16 Iteration: 30 Train loss: 0.133\n",
      "Epoch: 1/16 Iteration: 35 Train loss: 0.120\n",
      "Epoch: 1/16 Iteration: 40 Train loss: 0.095\n",
      "Epoch: 2/16 Iteration: 45 Train loss: 0.255\n",
      "Epoch: 2/16 Iteration: 50 Train loss: 0.252\n",
      "Val acc: 0.960\n",
      "Epoch: 2/16 Iteration: 55 Train loss: 0.074\n",
      "Epoch: 2/16 Iteration: 60 Train loss: 0.055\n",
      "Epoch: 2/16 Iteration: 65 Train loss: 0.044\n",
      "Epoch: 3/16 Iteration: 70 Train loss: 0.211\n",
      "Epoch: 3/16 Iteration: 75 Train loss: 0.046\n",
      "Val acc: 0.940\n",
      "Epoch: 3/16 Iteration: 80 Train loss: 0.057\n",
      "Epoch: 3/16 Iteration: 85 Train loss: 0.041\n",
      "Epoch: 4/16 Iteration: 90 Train loss: 0.186\n",
      "Epoch: 4/16 Iteration: 95 Train loss: 0.093\n",
      "Epoch: 4/16 Iteration: 100 Train loss: 0.049\n",
      "Val acc: 0.895\n",
      "Epoch: 4/16 Iteration: 105 Train loss: 0.040\n",
      "Epoch: 4/16 Iteration: 110 Train loss: 0.017\n",
      "Epoch: 5/16 Iteration: 115 Train loss: 0.116\n",
      "Epoch: 5/16 Iteration: 120 Train loss: 0.019\n",
      "Epoch: 5/16 Iteration: 125 Train loss: 0.014\n",
      "Val acc: 0.933\n",
      "Epoch: 5/16 Iteration: 130 Train loss: 0.003\n",
      "Epoch: 6/16 Iteration: 135 Train loss: 0.043\n",
      "Epoch: 6/16 Iteration: 140 Train loss: 0.046\n",
      "Epoch: 6/16 Iteration: 145 Train loss: 0.043\n",
      "Epoch: 6/16 Iteration: 150 Train loss: 0.039\n",
      "Val acc: 0.880\n",
      "Epoch: 7/16 Iteration: 155 Train loss: 0.045\n",
      "Epoch: 7/16 Iteration: 160 Train loss: 0.034\n",
      "Epoch: 7/16 Iteration: 165 Train loss: 0.018\n",
      "Epoch: 7/16 Iteration: 170 Train loss: 0.006\n",
      "Epoch: 7/16 Iteration: 175 Train loss: 0.002\n",
      "Val acc: 0.975\n",
      "Epoch: 8/16 Iteration: 180 Train loss: 0.031\n",
      "Epoch: 8/16 Iteration: 185 Train loss: 0.003\n",
      "Epoch: 8/16 Iteration: 190 Train loss: 0.015\n",
      "Epoch: 8/16 Iteration: 195 Train loss: 0.017\n",
      "Epoch: 9/16 Iteration: 200 Train loss: 0.014\n",
      "Val acc: 0.960\n",
      "Epoch: 9/16 Iteration: 205 Train loss: 0.004\n",
      "Epoch: 9/16 Iteration: 210 Train loss: 0.001\n",
      "Epoch: 9/16 Iteration: 215 Train loss: 0.001\n",
      "Epoch: 9/16 Iteration: 220 Train loss: 0.005\n",
      "Epoch: 10/16 Iteration: 225 Train loss: 0.012\n",
      "Val acc: 0.965\n",
      "Epoch: 10/16 Iteration: 230 Train loss: 0.000\n",
      "Epoch: 10/16 Iteration: 235 Train loss: 0.001\n",
      "Epoch: 10/16 Iteration: 240 Train loss: 0.000\n",
      "Epoch: 11/16 Iteration: 245 Train loss: 0.006\n",
      "Epoch: 11/16 Iteration: 250 Train loss: 0.000\n",
      "Val acc: 0.955\n",
      "Epoch: 11/16 Iteration: 255 Train loss: 0.000\n",
      "Epoch: 11/16 Iteration: 260 Train loss: 0.000\n",
      "Epoch: 12/16 Iteration: 265 Train loss: 0.001\n",
      "Epoch: 12/16 Iteration: 270 Train loss: 0.003\n",
      "Epoch: 12/16 Iteration: 275 Train loss: 0.000\n",
      "Val acc: 0.965\n",
      "Epoch: 12/16 Iteration: 280 Train loss: 0.000\n",
      "Epoch: 12/16 Iteration: 285 Train loss: 0.000\n",
      "Epoch: 13/16 Iteration: 290 Train loss: 0.000\n",
      "Epoch: 13/16 Iteration: 295 Train loss: 0.000\n",
      "Epoch: 13/16 Iteration: 300 Train loss: 0.000\n",
      "Val acc: 0.965\n",
      "Epoch: 13/16 Iteration: 305 Train loss: 0.007\n",
      "Epoch: 14/16 Iteration: 310 Train loss: 0.004\n",
      "Epoch: 14/16 Iteration: 315 Train loss: 0.000\n",
      "Epoch: 14/16 Iteration: 320 Train loss: 0.000\n",
      "Epoch: 14/16 Iteration: 325 Train loss: 0.000\n",
      "Val acc: 0.965\n",
      "Epoch: 14/16 Iteration: 330 Train loss: 0.000\n",
      "Epoch: 15/16 Iteration: 335 Train loss: 0.000\n",
      "Epoch: 15/16 Iteration: 340 Train loss: 0.000\n",
      "Epoch: 15/16 Iteration: 345 Train loss: 0.000\n",
      "Epoch: 15/16 Iteration: 350 Train loss: 0.000\n",
      "Val acc: 0.975\n"
     ]
    }
   ],
   "source": [
    "epochs = 16 # set Epochs ke 16\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y[:, None],\n",
    "                    keep_prob: 0.5,\n",
    "                    initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "            if iteration%25==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                    feed = {inputs_: x,\n",
    "                            labels_: y[:, None],\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "#     joblib.dump(sess, 'checkpoints/best.pkl')\n",
    "    saver.save(sess, \"checkpoints/sentiment.ckpt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Hamonangan\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from checkpoints\\sentiment.ckpt\n",
      "Test accuracy: 0.993\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "        feed = {inputs_: x,\n",
    "                labels_: y[:, None],\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
